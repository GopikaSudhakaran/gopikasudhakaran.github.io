<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Gopika Sudhakaran</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

<div class="sidebar">
  <h2>Gopika<br>Sudhakaran</h2>
  <nav>
    <a href="#about">About</a>
    <a href="#interests">Interests</a>
    <a href="#publications">Publications</a>
    <a href="#education">Education</a>
    <a href="#experience">Experience</a>
  </nav>
</div>

<div class="main">

  <section id="about" class="header">
    <img src="images/profile.jpeg" class="profile-pic" alt="Profile photo">
    <div class="title-block">
      <h1>Gopika Sudhakaran</h1>
      <h2>PhD Candidate in Visual Inference Lab and AIML Lab, TU Darmstadt</h2>
      <p class="links">
        <a href="https://github.com/GopikaSudhakaran">GitHub</a> ·
        <a href="https://www.linkedin.com/in/gopika-sudhakaran-7a289755/">LinkedIn</a> ·
        <a href="https://scholar.google.com/citations?user=QZS6FjoAAAAJ&hl=en">Google Scholar</a> ·
        gopika (dot) sudhakaran (at) tu-darmstadt (dot) de
      </p>
      <p>
        I am a PhD candidate at TU Darmstadt in the Visual Inference Lab (Stefan Roth) and the AIML Lab (Kristian Kersting). 
        My research focuses on vision–language models, scene understanding, and structured reasoning. 
        Currently, I am expanding this work into Neuro-Symbolic AI for embodied agents, investigating how to integrate open-vocabulary perception with logic 
        to achieve robust generalization in out-of-distribution (OOD) scenarios.
      </p>
    </div>
  </section>

  <section id="interests">
    <h3>Research Interests</h3>
    <div class="interest-grid">
      <div class="interest-chip">Vision–Language Models (VLMs)</div>
      <div class="interest-chip">Scene Understanding & Graphs</div>
      <div class="interest-chip">Neuro-Symbolic AI</div>
      <div class="interest-chip">Adversarial Robustness</div>
      <div class="interest-chip">Causal Reasoning</div>
      <div class="interest-chip">Active & Data-Efficient Learning</div>
      <div class="interest-chip">OOD Generalization</div>
      <div class="interest-chip">Embodied Agents</div>
    </div>
  </section>

  <section id="publications">
    <h3>Publications</h3>

    <div class="pub-card">
      <img src="images/teasers/art.png" class="teaser" alt="ART teaser">
      <div>
        <h4>ART: Adaptive Relation Tuning for Generalized Relation Prediction</h4>
        <p class="pub-meta">
          <b>Gopika Sudhakaran</b>, Hikaru Shindo, Patrick Schramowski, Simone Schaub-Meyer,
          Kristian Kersting, Stefan Roth · ICCV 2025
        </p>
        <p>Better OOD relation prediction with active-learning-inspired training for vision–language models.</p>
        <p class="pub-links">
          <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Sudhakaran_ART_Adaptive_Relation_Tuning_for_Generalized_Relation_Prediction_ICCV_2025_paper.pdf">PDF</a>
          <a href="https://github.com/visinf/ART">Code</a>
          <a href="https://www.youtube.com/watch?v=mqZXhgAA52w&t=2s">Video</a>
        </p>
      </div>
    </div>

    <div class="pub-card">
      <img src="images/teasers/deisam.png" class="teaser" alt="DeiSAM teaser">
      <div>
        <h4>DeiSAM: Segment Anything with Deictic Prompting</h4>
        <p class="pub-meta">
          Hikaru Shindo, Manuel Brack, <b>Gopika Sudhakaran</b>, Devendra Singh Dhami,
          Patrick Schramowski, Kristian Kersting · NeurIPS 2024
        </p>
        <p>Combining large language models and differentiable logic reasoning for deictic promptable segmentation.</p>
        <p class="pub-links">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5ddcfaad1cb72ce6f1a365e8f1ecf791-Paper-Conference.pdf">PDF</a>
          <a href="https://github.com/ml-research/deictic-segment-anything">Code</a>
        </p>
      </div>
    </div>

    <div class="pub-card">
      <img src="images/teasers/diagen.png" class="teaser" alt="DIAGen teaser">
      <div>
        <h4>DIAGen: Semantically Diverse Image Augmentation for Few-Shot Learning</h4>
        <p class="pub-meta">
          Tobias Lingenberg, Markus Reuter, <b>Gopika Sudhakaran</b>, Dominik Gojny,
          Stefan Roth, Simone Schaub-Meyer · GCPR 2024
        </p>
        <p>Generative, semantically diverse augmentations that improve few-shot robustness and downstream classifiers.</p>
        <p class="pub-links">
          <a href="https://arxiv.org/pdf/2408.14584">PDF</a>
          <a href="https://github.com/visinf/DIAGen">Code</a>
        </p>
      </div>
    </div>

    <div class="pub-card">
      <img src="images/teasers/veto.png" class="teaser" alt="VRT teaser">
      <div>
        <h4>Vision Relation Transformer for Unbiased Scene Graph Generation</h4>
        <p class="pub-meta">
          <b>Gopika Sudhakaran</b>, Devendra Singh Dhami, Kristian Kersting, Stefan Roth · ICCV 2023
        </p>
        <p>Debiased relation encoding and training for long-tail scene graph generation with strong gains.</p>
        <p class="pub-links">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakaran_Vision_Relation_Transformer_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.pdf">PDF</a>
          <a href="https://github.com/visinf/veto">Code</a>
          <a href="https://www.youtube.com/watch?v=N4YqmfDY-t0">Video</a>
        </p>
      </div>
    </div>

  </section>

  <section id="education">
    <h3>Education</h3>

    <div class="edu-item">
      <h4>PhD in Computer Science (CV and AI/ML)</h4>
      <p class="edu-meta">TU Darmstadt, Germany · Oct 2021 – Present</p>
      <p>Advisors: Prof. Stefan Roth & Prof. Kristian Kersting. Research on vision–language models, scene understanding and reasoning.</p>
    </div>

    <div class="edu-item">
      <h4>M.Sc. in Data Analytics</h4>
      <p class="edu-meta">University of Hildesheim, Germany · Apr 2019 – Aug 2021 · Final grade: 1.2</p>
      <p>Master Thesis (Bosch Corporate Research): Learning to Active Learn for Pedestrian Detection (Grade: 1.0).</p>
    </div>

    <div class="edu-item">
      <h4>B.E. in Computer Science & Engineering</h4>
      <p class="edu-meta">PSG College of Technology, India · Jul 2011 – May 2015 · Final grade: 8.3/10</p>
      <p>Bachelor Thesis (McKinsey & Company): Internal tool estimating success of transformation projects (Grade: 10/10).</p>
    </div>
  </section>

  <section id="experience">
    <h3>Work Experience</h3>

    <div class="exp-item">
      <h4>Research Intern — Bosch Corporate Research</h4>
      <p class="edu-meta">Renningen, Germany · Jan 2021 – Jun 2021</p>
      <p>Worked on “Learning to Active Learn” for pedestrian detection, performing calibration and robustness evaluation on real-world datasets.</p>
    </div>

    <div class="exp-item">
      <h4>Machine Learning Intern, Highly Automated Driving — Bosch GmbH</h4>
      <p class="edu-meta">Abstatt, Germany · May 2020 – Oct 2020</p>
      <p>Analyzed adversarial robustness of deep models for automated driving, implementing PGD, FGSM and LaVAN attacks on real and synthetic video data.</p>
    </div>

    <div class="exp-item">
      <h4>Research Assistant — University of Hildesheim</h4>
      <p class="edu-meta">Hildesheim, Germany · Oct 2019 – Apr 2020</p>
      <p>Worked on object detection and similarity retrieval in large cartoon/sketch datasets using Siamese and ResNet-based methods.</p>
    </div>

    <div class="exp-item">
      <h4>Data Analyst — McKinsey & Company</h4>
      <p class="edu-meta">Chennai, India · Jul 2015 – Aug 2018</p>
      <p>Developed product recommendation tools, cross-sell forecasting models, KPI tracking systems, and analytics modules for global financial clients.</p>
    </div>

    <div class="exp-item">
      <h4>Data Analyst Intern — McKinsey & Company</h4>
      <p class="edu-meta">Chennai, India · Dec 2014 – May 2015</p>
    </div>
  </section>

  <footer>
    <p>© Gopika Sudhakaran</p>
  </footer>

</div>

</body>
</html>



